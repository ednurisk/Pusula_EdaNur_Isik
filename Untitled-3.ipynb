{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9e57c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Talent Academy — Veri Temizleme ve EDA Pipeline\n",
    "------------------------------------------------\n",
    "Bu dosya, adım adım temizleme, normalizasyon, eksik doldurma,\n",
    "encoding ve basit EDA işleri için modüler fonksiyonlar içerir.\n",
    "\n",
    "Kullanım:\n",
    "    from talent_academy_pipeline import main_pipeline\n",
    "    main_pipeline(file_path=\"/path/to/Talent_Academy_Case_DT_2025.xlsx\",\n",
    "                  save_path=None, perform_eda=True, scale_uygulama=True)\n",
    "\n",
    "Notlar:\n",
    "- Orijinal dosyayı otomatik üzerine yazmamak için default olarak\n",
    "  temizlenmiş dosyayı \"*_cleaned.xlsx\" olarak kaydeder.\n",
    "- Fonksiyonlar bağımsızdır; ihtiyacına göre sırayı değiştirebilirsin.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import ast\n",
    "import math\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# Görselleştirme sadece isteğe bağlı\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ==================================================================\n",
    "# Yardımcı Fonksiyonlar\n",
    "# ==================================================================\n",
    "\n",
    "def load_data(file_path: str, sheet_name: str = \"Sheet1\") -> pd.DataFrame:\n",
    "    \"\"\"Excel dosyasını yükler ve DataFrame döndürür.\"\"\"\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    print(f\"Veri yüklendi: {df.shape[0]} satır, {df.shape[1]} sütun\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_data(df: pd.DataFrame, save_path: Optional[str]):\n",
    "    \"\"\"DataFrame'i kaydeder. Eğer save_path None ise dosya adı otomatik oluşturulur.\"\"\"\n",
    "    if save_path is None:\n",
    "        save_path = \"Talent_Academy_Case_DT_2025_cleaned.xlsx\"\n",
    "    df.to_excel(save_path, index=False)\n",
    "    print(f\"Veri kaydedildi: {save_path}\")\n",
    "\n",
    "\n",
    "# Daha güvenli boş kontrolü\n",
    "def is_empty(val) -> bool:\n",
    "    try:\n",
    "        if pd.isna(val):\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if isinstance(val, str):\n",
    "        return val.strip() == \"\"\n",
    "\n",
    "    if isinstance(val, (list, tuple, np.ndarray)):\n",
    "        return len(val) == 0\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# 1) Sayısal dönüşümler\n",
    "# ==================================================================\n",
    "\n",
    "def convert_to_numeric(df: pd.DataFrame, numeric_cols_info: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"Metin içeren süre gibi sütunlardaki birimleri temizleyip sayıya çevirir.\n",
    "\n",
    "    numeric_cols_info: {\"KolonAdi\": \"silinecek metin\"}\n",
    "    \"\"\"\n",
    "    for col, remove_text in numeric_cols_info.items():\n",
    "        if col not in df.columns:\n",
    "            print(f\"Uyarı: {col} bulunamadı, atlanıyor.\")\n",
    "            continue\n",
    "        df[col] = df[col].astype(str).str.replace(remove_text, \"\", regex=False).str.strip()\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        # NaN'ları 0 ile doldurma davranışı isteğe bağlı — default dolduruluyor\n",
    "        df[col].fillna(0, inplace=True)\n",
    "    print(\"Sayısal dönüşümler tamamlandı.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# 2) Grup bazlı eksik değer doldurma (KronikHastalik, Alerji gibi değişmeyen bilgiler)\n",
    "# ==================================================================\n",
    "\n",
    "def fill_groupwise_static_columns(df: pd.DataFrame, group_cols: List[str],\n",
    "                                  fill_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Her hasta/grup için eğer o grupta tek tip değer varsa eksikleri doldurur.\n",
    "\n",
    "    Örneğin: group_cols = [\"HastaNo\", \"Yas\", \"Cinsiyet\", \"KanGrubu\"]\n",
    "                 fill_cols = [\"KronikHastalik\", \"Alerji\"]\n",
    "    \"\"\"\n",
    "    # Önce grup anahtarı ile grupla\n",
    "    if not set(group_cols).issubset(df.columns):\n",
    "        missing = list(set(group_cols) - set(df.columns))\n",
    "        raise ValueError(f\"Gruplama için gerekli kolon bulunamadı: {missing}\")\n",
    "\n",
    "    for fill_col in fill_cols:\n",
    "        if fill_col not in df.columns:\n",
    "            print(f\"Uyarı: Doldurulacak kolon bulunamadı: {fill_col} -> atlanıyor\")\n",
    "            continue\n",
    "\n",
    "        # Grup bazında unique non-empty değerleri bul\n",
    "        def get_unique_non_empty(series):\n",
    "            normalized_vals = []\n",
    "            for v in series.dropna():\n",
    "                if is_empty(v):\n",
    "                    continue\n",
    "                # Eğer listeyse stringe çevir\n",
    "                if isinstance(v, list):\n",
    "                    v = \", \".join(map(str, v))\n",
    "                normalized_vals.append(v)\n",
    "            return list(pd.unique(normalized_vals))  # Tekrarlayanları kaldır\n",
    "\n",
    "        # Özet tablosu oluştur\n",
    "        grp = df.groupby(group_cols)[fill_col].apply(lambda s: get_unique_non_empty(s))\n",
    "\n",
    "        # Gruplara göre doldur\n",
    "        filled = 0\n",
    "        for key, vals in grp.items():\n",
    "            if len(vals) == 1:\n",
    "                # key tuple olarak gelebilir, bunu mask'e çevir\n",
    "                if not isinstance(key, tuple):\n",
    "                    key = (key,)\n",
    "                mask = True\n",
    "                for col_name, v in zip(group_cols, key):\n",
    "                    mask &= (df[col_name] == v) if not pd.isna(v) else pd.isna(df[col_name])\n",
    "                # Bu mask içindeki boş hücreleri doldur\n",
    "                mask_to_fill = mask & (df[fill_col].isna() | df[fill_col].apply(is_empty))\n",
    "                df.loc[mask_to_fill, fill_col] = vals[0]\n",
    "                filled += mask_to_fill.sum()\n",
    "\n",
    "        print(f\"{fill_col} için doldurulan kayıt sayısı: {filled}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# 3) Metin normalizasyonu (virgül ile ayrılmış listeleri auto-normalize etme)\n",
    "# ==================================================================\n",
    "\n",
    "auto_normalization_dict: Dict[str, str] = {}\n",
    "\n",
    "\n",
    "def clean_text_basic(text: str) -> str:\n",
    "    text = str(text).strip()\n",
    "    if any(char.isdigit() for char in text):\n",
    "        return text\n",
    "    text_lower = text.lower()\n",
    "    # Türkçe karakterleri koruyarak noktalama işaretlerini temizle\n",
    "    text_lower = re.sub(r\"[^\\w\\sçğıöşüÇĞİÖŞÜ-]\", \"\", text_lower)\n",
    "    text_lower = re.sub(r\"\\s+\", \" \", text_lower).strip()\n",
    "    return text_lower\n",
    "\n",
    "\n",
    "def normalize_auto(text: str, threshold: int = 90) -> Optional[str]:\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    cleaned = clean_text_basic(text)\n",
    "    if any(char.isdigit() for char in cleaned):\n",
    "        return cleaned\n",
    "    if cleaned in auto_normalization_dict:\n",
    "        return auto_normalization_dict[cleaned]\n",
    "    if auto_normalization_dict:\n",
    "        result = process.extractOne(cleaned, list(auto_normalization_dict.keys()), scorer=fuzz.ratio)\n",
    "        if result:\n",
    "            match, score, *_ = result\n",
    "            if score >= threshold:\n",
    "                auto_normalization_dict[cleaned] = auto_normalization_dict[match]\n",
    "                return auto_normalization_dict[match]\n",
    "    auto_normalization_dict[cleaned] = cleaned\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def split_and_normalize_column_auto(df: pd.DataFrame, kolon_adi: str) -> pd.DataFrame:\n",
    "    \"\"\"Virgülle ayrılmış stringleri listeye çevirir ve normalize eder.\"\"\"\n",
    "    if kolon_adi not in df.columns:\n",
    "        print(f\"Uyarı: {kolon_adi} bulunamadı, atlandı.\")\n",
    "        return df\n",
    "\n",
    "    def process_value(value):\n",
    "        if pd.isna(value):\n",
    "            return None\n",
    "        parts = [p.strip() for p in str(value).split(\",\") if p.strip()]\n",
    "        parts = [normalize_auto(p) for p in parts]\n",
    "        # unique ve sıra koru\n",
    "        parts = list(dict.fromkeys([p for p in parts if p is not None]))\n",
    "        return parts if parts else None\n",
    "\n",
    "    df[kolon_adi] = df[kolon_adi].apply(process_value)\n",
    "    print(f\"{kolon_adi} normalize edildi (liste formatı).\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_text_columns(df: pd.DataFrame, kolonlar: List[str]) -> pd.DataFrame:\n",
    "    for k in kolonlar:\n",
    "        df = split_and_normalize_column_auto(df, k)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# 4) TedaviAdi özel ayrıştırması ve normalize edilmesi\n",
    "# ==================================================================\n",
    "\n",
    "def normalize_text_unicode(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    s = \" \".join(s.split())\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.casefold()\n",
    "    return s\n",
    "\n",
    "\n",
    "def split_tedavi(value: str) -> List[str]:\n",
    "    if pd.isna(value):\n",
    "        return []\n",
    "    s = str(value).strip()\n",
    "    if \"-\" in s:\n",
    "        parts = [p.strip() for p in s.split(\"-\")]\n",
    "        if parts and parts[-1].isdigit():\n",
    "            return [s]\n",
    "        return [p for p in parts if p]\n",
    "    if \"+\" in s:\n",
    "        if any(word in s.casefold().split() for word in [\"sağ\", \"sol\"]):\n",
    "            return [s]\n",
    "        parts = [p.strip() for p in s.split(\"+\")]\n",
    "        if parts and parts[-1].isdigit():\n",
    "            return [s]\n",
    "        return [p for p in parts if p]\n",
    "    return [s]\n",
    "\n",
    "\n",
    "def normalize_tedavi_column(df: pd.DataFrame, kolon_adi: str = \"TedaviAdi\") -> pd.DataFrame:\n",
    "    if kolon_adi not in df.columns:\n",
    "        print(f\"Uyarı: {kolon_adi} bulunamadı, atlanıyor\")\n",
    "        return df\n",
    "\n",
    "    df[kolon_adi] = df[kolon_adi].apply(lambda value: \" \".join([normalize_text_unicode(x) for x in split_tedavi(value)]))\n",
    "    print(f\"{kolon_adi} normalize edildi.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# 5) Liste kolonlarını normalize etme (case-preserving ilk versiyon)\n",
    "# ==================================================================\n",
    "\n",
    "def normalize_list_cell(cell):\n",
    "    if cell is None or (isinstance(cell, float) and math.isnan(cell)):\n",
    "        return cell\n",
    "    try:\n",
    "        if isinstance(cell, str):\n",
    "            if cell.startswith('[') and cell.endswith(']'):\n",
    "                items = ast.literal_eval(cell)\n",
    "            else:\n",
    "                items = [x.strip() for x in cell.split(',') if x.strip()]\n",
    "        elif isinstance(cell, (list, np.ndarray)):\n",
    "            items = list(cell)\n",
    "        else:\n",
    "            return cell\n",
    "\n",
    "        norm_map = {}\n",
    "        for item in items:\n",
    "            if item is None or (isinstance(item, float) and math.isnan(item)):\n",
    "                continue\n",
    "            key = str(item).strip().lower()\n",
    "            if key not in norm_map:\n",
    "                # Orijinal formda sakla (ilk görüleni)\n",
    "                norm_map[key] = item.strip() if isinstance(item, str) else item\n",
    "        new_list = list(norm_map.values())\n",
    "        return new_list\n",
    "    except Exception:\n",
    "        return cell\n",
    "\n",
    "\n",
    "def normalize_list_columns(df: pd.DataFrame, kolonlar: List[str]) -> pd.DataFrame:\n",
    "    total_changes = 0\n",
    "    for col in kolonlar:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Uyarı: {col} bulunamadı, atlanıyor\")\n",
    "            continue\n",
    "        before = df[col].copy()\n",
    "        df[col] = df[col].apply(normalize_list_cell)\n",
    "        total_changes += (before != df[col]).sum()\n",
    "    print(f\"Toplam değişiklik yapılan satır sayısı: {total_changes}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# 6) Encoding ve Feature Engineering\n",
    "# ==================================================================\n",
    "\n",
    "def encode_cinsiyet(df: pd.DataFrame, column: str = \"Cinsiyet\") -> pd.DataFrame:\n",
    "    if column not in df.columns:\n",
    "        print(f\"Uyarı: {column} bulunamadı.\")\n",
    "        return df\n",
    "    mapping = {\"Kadın\": 0, \"Erkek\": 1, \"Kadin\": 0, \"E\": 1, \"K\": 0}\n",
    "    df[column] = df[column].map(mapping).fillna(df[column])\n",
    "    # Eğer hala stringler varsa (ör. küçük harf) bir daha map yap\n",
    "    df[column] = df[column].replace({\"kadın\": 0, \"erkek\": 1})\n",
    "    print(\"Cinsiyet encode edildi (kısmi map).\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_kan_grubu_and_rh(df: pd.DataFrame, column: str = \"KanGrubu\") -> pd.DataFrame:\n",
    "    if column not in df.columns:\n",
    "        print(f\"Uyarı: {column} bulunamadı.\")\n",
    "        return df\n",
    "\n",
    "    def split_kan(s):\n",
    "        if pd.isna(s):\n",
    "            return (pd.NA, pd.NA)\n",
    "        s = str(s).strip()\n",
    "        # Örn \"A Rh+\" veya \"A+\" veya \"0 Rh-\" gibi formatlar olabilir\n",
    "        # İlk olarak Rh kısmını ayıkla\n",
    "        rh_match = re.search(r\"(rh\\+|rh\\-|\\+|-)\", s, flags=re.IGNORECASE)\n",
    "        rh = pd.NA\n",
    "        if rh_match:\n",
    "            token = rh_match.group(0)\n",
    "            if token.lower().startswith('rh'):\n",
    "                rh = token.capitalize()\n",
    "            else:\n",
    "                rh = 'Rh+' if token == '+' else 'Rh-'\n",
    "            # kan türünü çıkar\n",
    "            kan = re.sub(re.escape(token), '', s, flags=re.IGNORECASE).strip()\n",
    "        else:\n",
    "            # eğer A+, B- gibi boşluk yoksa\n",
    "            if len(s) <= 3 and ('+' in s or '-' in s):\n",
    "                kan = s.replace('+', '').replace('-', '').strip()\n",
    "                rh = 'Rh+' if '+' in s else ('Rh-' if '-' in s else pd.NA)\n",
    "            else:\n",
    "                kan = s\n",
    "        return (kan, rh)\n",
    "\n",
    "    df[['KanGrubuTürü', 'RhFaktörü']] = df[column].apply(lambda s: pd.Series(split_kan(s)))\n",
    "    print(\"KanGrubuTürü ve RhFaktörü sütunları oluşturuldu.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def one_hot_encode_uyruk(df: pd.DataFrame, column: str = \"Uyruk\") -> pd.DataFrame:\n",
    "    if column not in df.columns:\n",
    "        print(f\"Uyarı: {column} bulunamadı.\")\n",
    "        return df\n",
    "    dummies = pd.get_dummies(df[column], prefix='Uyruk')\n",
    "    df = pd.concat([df.drop(columns=[column]), dummies], axis=1)\n",
    "    print(\"Uyruk için one-hot encoding uygulandı.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def age_group_encoding_col(df: pd.DataFrame, column: str = 'Yas', new_column: str = 'YasGrubu') -> pd.DataFrame:\n",
    "    if column not in df.columns:\n",
    "        print(f\"Uyarı: {column} bulunamadı.\")\n",
    "        return df\n",
    "\n",
    "    def age_group(age):\n",
    "        try:\n",
    "            age = float(age)\n",
    "        except Exception:\n",
    "            return pd.NA\n",
    "        if age <= 14:\n",
    "            return 0\n",
    "        elif age <= 24:\n",
    "            return 1\n",
    "        elif age <= 64:\n",
    "            return 2\n",
    "        elif age <= 79:\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "\n",
    "    df[new_column] = df[column].apply(age_group).astype('Int64')\n",
    "    print(f\"{new_column} oluşturuldu.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def scale_column(df: pd.DataFrame, column: str = 'UygulamaSuresi') -> pd.DataFrame:\n",
    "    if column not in df.columns:\n",
    "        print(f\"Uyarı: {column} bulunamadı.\")\n",
    "        return df\n",
    "    scaler = StandardScaler()\n",
    "    # reshape gibi hatalara karşı\n",
    "    vals = df[[column]].astype(float).fillna(0)\n",
    "    df[column] = scaler.fit_transform(vals)\n",
    "    print(f\"{column} ölçeklendi (z-score).\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# 7) EDA fonksiyonları (opsiyonel görselleştirme)\n",
    "# ==================================================================\n",
    "\n",
    "def eda_summary(df: pd.DataFrame, n_head: int = 10):\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    print(\"===== VERİ SETİ GENEL BİLGİLER =====\")\n",
    "    print(df.info())\n",
    "    print('\\nİlk {} kayıt:'.format(n_head))\n",
    "    print(df.head(n_head))\n",
    "    print('\\n===== EKSİK VERİLER (satır bazında) =====')\n",
    "    miss = df.isnull().sum()\n",
    "    miss_pct = (miss / len(df) * 100).round(2)\n",
    "    print(pd.concat([miss, miss_pct], axis=1, keys=['missing_count', 'missing_pct']).sort_values('missing_count', ascending=False))\n",
    "    print('\\n===== TEMEL İSTATİSTİKSEL BİLGİLER =====')\n",
    "    print(df.describe(include='all'))\n",
    "\n",
    "\n",
    "def plot_numeric_distributions(df: pd.DataFrame, exclude_cols: List[str] = None):\n",
    "    exclude_cols = exclude_cols or []\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
    "    for col in numeric_cols:\n",
    "        plt.figure(figsize=(6, 3.5))\n",
    "        sns.histplot(df[col].dropna(), kde=True, bins=20)\n",
    "        plt.title(f\"{col} dağılımı\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_categorical_distributions(df: pd.DataFrame, categorical_cols: List[str]):\n",
    "    for col in categorical_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        plt.figure(figsize=(6, 3.5))\n",
    "        order = df[col].value_counts().index\n",
    "        sns.countplot(y=df[col], order=order)\n",
    "        plt.title(f\"{col} kategorik dağılımı\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def correlation_heatmap(df: pd.DataFrame, exclude_cols: List[str] = None):\n",
    "    exclude_cols = exclude_cols or []\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
    "    if not numeric_cols:\n",
    "        print(\"Sayısal kolon yok, korelasyon çizilemez.\")\n",
    "        return\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Sayısal Değişkenler Korelasyon Matrisi')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# Ana Pipeline fonksiyonu\n",
    "# ==================================================================\n",
    "\n",
    "def main_pipeline(file_path: str,\n",
    "                  save_path: Optional[str] = None,\n",
    "                  perform_eda: bool = True,\n",
    "                  scale_uygulama: bool = True,\n",
    "                  one_hot_uyruk: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Pipeline'i çalıştırır ve temizlenmiş DataFrame döndürür.\n",
    "\n",
    "    Args:\n",
    "        file_path: Girdi Excel dosyası\n",
    "        save_path: Kaydetme yolu (None ise default cleaned.xlsx)\n",
    "        perform_eda: Eğer True ise özet ve grafikler gösterilir\n",
    "        scale_uygulama: UygulamaSuresi sütununu z-score ile ölçekle\n",
    "        one_hot_uyruk: Uyruk için one-hot uygula\n",
    "    \"\"\"\n",
    "    df = load_data(file_path)\n",
    "\n",
    "    # EDA\n",
    "    if perform_eda:\n",
    "        eda_summary(df)\n",
    "\n",
    "    # 1) sayısal dönüşümler\n",
    "    numeric_cols_info = {\"TedaviSuresi\": \" Seans\", \"UygulamaSuresi\": \" Dakika\"}\n",
    "    df = convert_to_numeric(df, numeric_cols_info)\n",
    "\n",
    "    # 2) metin kolonlarını normalize et (list ve virgül parçalama)\n",
    "    text_cols = [c for c in [\"KronikHastalik\", \"Bolum\", \"Alerji\", \"Tanilar\", \"UygulamaYerleri\"] if c in df.columns]\n",
    "    df = normalize_text_columns(df, text_cols)\n",
    "\n",
    "    # 3) TedaviAdi özel normalize\n",
    "    if 'TedaviAdi' in df.columns:\n",
    "        df = normalize_tedavi_column(df, 'TedaviAdi')\n",
    "\n",
    "    # 4) Liste kolonlarını normalize et\n",
    "    list_cols = [c for c in [\"TedaviAdi\", \"KronikHastalik\", \"Tanilar\"] if c in df.columns]\n",
    "    df = normalize_list_columns(df, list_cols)\n",
    "\n",
    "    # 5) grup bazlı doldurma (KronikHastalik ve Alerji)\n",
    "    group_cols = [c for c in [\"HastaNo\", \"Yas\", \"Cinsiyet\", \"KanGrubu\"] if c in df.columns]\n",
    "    fill_cols = [c for c in [\"KronikHastalik\", \"Alerji\"] if c in df.columns]\n",
    "    if group_cols and fill_cols:\n",
    "        df = fill_groupwise_static_columns(df, group_cols=group_cols, fill_cols=fill_cols)\n",
    "\n",
    "    # 6) Cinsiyet encoding\n",
    "    if 'Cinsiyet' in df.columns:\n",
    "        df = encode_cinsiyet(df, 'Cinsiyet')\n",
    "\n",
    "    # 7) KanGrubu'dan tür ve RH çıkart\n",
    "    if 'KanGrubu' in df.columns:\n",
    "        df = extract_kan_grubu_and_rh(df, 'KanGrubu')\n",
    "\n",
    "    # 8) Uyruk one-hot\n",
    "    if one_hot_uyruk and 'Uyruk' in df.columns:\n",
    "        df = one_hot_encode_uyruk(df, 'Uyruk')\n",
    "\n",
    "    # 9) Yaş grubu\n",
    "    if 'Yas' in df.columns:\n",
    "        df = age_group_encoding_col(df, 'Yas', 'YasGrubu')\n",
    "\n",
    "    # 10) UygulamaSuresi ölçekle\n",
    "    if scale_uygulama and 'UygulamaSuresi' in df.columns:\n",
    "        df = scale_column(df, 'UygulamaSuresi')\n",
    "\n",
    "    # Opsiyonel EDA grafiklerini tekrar göster\n",
    "    if perform_eda:\n",
    "        # Bazı görsellerin çok sık çıkmaması için sadece özet seçeneği\n",
    "        try:\n",
    "            plot_numeric_distributions(df, exclude_cols=['HastaNo'])\n",
    "            cat_cols = [c for c in ['Cinsiyet', 'KanGrubu', 'Uyruk', 'Bolum', 'UygulamaSuresi', 'TedaviSuresi'] if c in df.columns]\n",
    "            plot_categorical_distributions(df, cat_cols)\n",
    "            correlation_heatmap(df, exclude_cols=['HastaNo'])\n",
    "        except Exception as e:\n",
    "            print(f\"Görselleştirme sırasında hata: {e}\")\n",
    "\n",
    "    # Son kaydet\n",
    "    save_data(df, save_path)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48548bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri yüklendi: 2235 satır, 13 sütun\n",
      "Sayısal dönüşümler tamamlandı.\n",
      "KronikHastalik normalize edildi (liste formatı).\n",
      "Bolum normalize edildi (liste formatı).\n",
      "Alerji normalize edildi (liste formatı).\n",
      "Tanilar normalize edildi (liste formatı).\n",
      "UygulamaYerleri normalize edildi (liste formatı).\n",
      "TedaviAdi normalize edildi.\n",
      "Toplam değişiklik yapılan satır sayısı: 2921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pro\\AppData\\Local\\Temp\\ipykernel_5180\\2358278120.py:85: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\pro\\AppData\\Local\\Temp\\ipykernel_5180\\2358278120.py:85: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\pro\\AppData\\Local\\Temp\\ipykernel_5180\\2358278120.py:121: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  return list(pd.unique(normalized_vals))  # Tekrarlayanları kaldır\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KronikHastalik için doldurulan kayıt sayısı: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pro\\AppData\\Local\\Temp\\ipykernel_5180\\2358278120.py:121: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  return list(pd.unique(normalized_vals))  # Tekrarlayanları kaldır\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alerji için doldurulan kayıt sayısı: 0\n",
      "Cinsiyet encode edildi (kısmi map).\n",
      "KanGrubuTürü ve RhFaktörü sütunları oluşturuldu.\n",
      "Uyruk için one-hot encoding uygulandı.\n",
      "YasGrubu oluşturuldu.\n",
      "UygulamaSuresi ölçeklendi (z-score).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pro\\AppData\\Local\\Temp\\ipykernel_5180\\2358278120.py:303: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[column] = df[column].map(mapping).fillna(df[column])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri kaydedildi: Talent_Academy_Case_DT_2025_cleaned.xlsx\n",
      "Pipeline çalıştı. Temizlenmiş DataFrame döndürüldü.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Örnek çağrı (kendi yolunu buraya yaz)\n",
    "    df_cleaned = main_pipeline(file_path=r\"Talent_Academy_Data.xlsx\",\n",
    "                               save_path=None,\n",
    "                               perform_eda=False,\n",
    "                               scale_uygulama=True,\n",
    "                               one_hot_uyruk=True)\n",
    "    print('Pipeline çalıştı. Temizlenmiş DataFrame döndürüldü.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
